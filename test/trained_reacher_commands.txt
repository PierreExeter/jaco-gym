
# pre trained agents: work OK
python enjoy.py --algo sac --env ReacherBulletEnv-v0 --folder trained_agents/ -n 5000
python enjoy.py --algo ppo2 --env ReacherBulletEnv-v0 --folder trained_agents/ -n 5000



# train agent myself
python train.py --algo ppo2 --env ReacherBulletEnv-v0 --eval-freq 10000 --eval-episodes 10 --save-freq 100000
python train.py --algo ppo2 --env Reacher2Dof-v0 --eval-freq 10000 --eval-episodes 10 --save-freq 100000
python train.py --algo sac --env ReacherBulletEnv-v0 --eval-freq 10000 --eval-episodes 10 --save-freq 100000


# enjoy agent trained by myself


1500 time steps because one episode last for 150 time steps and I want to evaluate for 10 episodes

python enjoy.py --algo ppo2 --env ReacherBulletEnv-v0 -f logs/ --exp-id 0 -n 1500
python enjoy.py --algo ppo2 --env ReacherBulletEnv-v0 -f logs/ --exp-id 0 -n 1500

python enjoy.py --algo ppo2 --env Reacher2Dof-v0 -f logs/ --exp-id 0 -n 1500




------------ PENDULUM-V0------------

# enjoy pre trained agents
python enjoy.py --algo a2c --env Pendulum-v0 --folder trained_agents/ -n 5000
python enjoy.py --algo acktr --env Pendulum-v0 --folder trained_agents/ -n 5000
python enjoy.py --algo ppo2 --env Pendulum-v0 --folder trained_agents/ -n 5000
python enjoy.py --algo ddpg --env Pendulum-v0 --folder trained_agents/ -n 5000
python enjoy.py --algo sac --env Pendulum-v0 --folder trained_agents/ -n 5000
python enjoy.py --algo td3 --env Pendulum-v0 --folder trained_agents/ -n 5000
python enjoy.py --algo trpo --env Pendulum-v0 --folder trained_agents/ -n 5000


# optimise hyperparameters
python train.py --algo a2c --env Pendulum-v0 -n 100000 -optimize --n-trials 100 --n-jobs -1 --sampler tpe --pruner median
python train.py --algo acktr --env Pendulum-v0 -n 10000 -optimize --n-trials 100 --n-jobs -1 --sampler tpe --pruner median
python train.py --algo ppo2 --env Pendulum-v0 -n 10000 -optimize --n-trials 100 --n-jobs -1 --sampler tpe --pruner median
python train.py --algo ddpg --env Pendulum-v0 -n 10000 -optimize --n-trials 100 --n-jobs -1 --sampler tpe --pruner median
python train.py --algo sac --env Pendulum-v0 -n 10000 -optimize --n-trials 100 --n-jobs -1 --sampler tpe --pruner median
python train.py --algo td3 --env Pendulum-v0 -n 10000 -optimize --n-trials 100 --n-jobs -1 --sampler tpe --pruner median
python train.py --algo ppo2 --env Pendulum-v0 -n 10000 -optimize --n-trials 100 --n-jobs -1 --sampler tpe --pruner median
python train.py --algo trpo --env Pendulum-v0 -n 10000 -optimize --n-trials 100 --n-jobs -1 --sampler tpe --pruner median


# train agent myself
python train.py --algo a2c --env Pendulum-v0 -n 10000 --eval-freq 10000 --eval-episodes 10 --save-freq 100000 --seed 0
python train.py --algo acktr --env Pendulum-v0 -n 10000 --eval-freq 10000 --eval-episodes 10 --save-freq 100000 --seed 0
python train.py --algo ppo2 --env Pendulum-v0 -n 10000 --eval-freq 10000 --eval-episodes 10 --save-freq 100000 --seed 0
python train.py --algo ddpg --env Pendulum-v0 -n 10000 --eval-freq 10000 --eval-episodes 10 --save-freq 100000 --seed 0
python train.py --algo sac --env Pendulum-v0 -n 10000 --eval-freq 10000 --eval-episodes 10 --save-freq 100000 --seed 0
python train.py --algo td3 --env Pendulum-v0 -n 10000 --eval-freq 10000 --eval-episodes 10 --save-freq 100000 --seed 0
python train.py --algo trpo --env Pendulum-v0 -n 10000 --eval-freq 10000 --eval-episodes 10 --save-freq 100000 --seed 0

python train.py --algo ppo2 --env JacoGazebo-v1 -n 100000 --seed 0 --log-folder logs/ppo2/JacoGazebo-v1_100000/ &> submission_log/log_ppo_jaco.run


# train with seeds
# choose hyperparameters in hyperparameters/a2c.yml and run this
# see run_experiments_pendulum.sh


# enjoy agent trained by myself
python enjoy.py --algo a2c --env Pendulum-v0 -f logs/ --exp-id 0 --no-render -n 2000
python enjoy.py --algo acktr --env Pendulum-v0 -f logs/ --exp-id 0 --no-render -n 2000
python enjoy.py --algo ppo2 --env Pendulum-v0 -f logs/ --exp-id 0 --no-render -n 2000
python enjoy.py --algo ddpg --env Pendulum-v0 -f logs/ --exp-id 0 --no-render -n 2000
python enjoy.py --algo sac --env Pendulum-v0 -f logs/ --exp-id 0 --no-render -n 2000
python enjoy.py --algo td3 --env Pendulum-v0 -f logs/ --exp-id 0 --no-render -n 2000
python enjoy.py --algo trpo --env Pendulum-v0 -f logs/ --exp-id 0 --no-render -n 2000



# plot learning curve and compute training stats
plot_results.py   # change log dir



# evaluate an exeriment 
# eval for 100 episodes


python enjoy.py --algo a2c --env Pendulum-v0 -f logs/a2c/Pendulum-v0_Env_1/ --exp-id 1 --no-render -n 20000
python plot_results.py -f logs/a2c/Pendulum-v0_Env_1/a2c/Pendulum-v0_1/


# hyperparameters from training are automatically laoded from config.yml, I don't need to match the same as in hyperparameters/a2c.yml
./get_results_exp.sh

python plot_experiment.py -f logs/a2c/Pendulum-v0_Env_1/a2c/ --env Pendulum-v0
python plot_experiment.py -f logs/ppo2/Reacher2Dof-v0_Env_1_norm/ppo2/ --env Reacher2Dof-v0

python plot_experiment.py -f logs/a2c/Reacher2Dof-v0_Env_8_norm/a2c/ --env Reacher2Dof-v0
python plot_experiment.py -f logs/acktr/Reacher2Dof-v0_Env_8_norm/acktr/ --env Reacher2Dof-v0
python plot_experiment.py -f logs/ddpg/Reacher2Dof-v0_Env_8_norm/ddpg/ --env Reacher2Dof-v0
python plot_experiment.py -f logs/ppo2/Reacher2Dof-v0_Env_8_norm/ppo2/ --env Reacher2Dof-v0
python plot_experiment.py -f logs/sac/Reacher2Dof-v0_Env_8_norm/sac/ --env Reacher2Dof-v0
python plot_experiment.py -f logs/td3/Reacher2Dof-v0_Env_8_norm/td3/ --env Reacher2Dof-v0
python plot_experiment.py -f logs/trpo/Reacher2Dof-v0_Env_8_norm/trpo/ --env Reacher2Dof-v0



python plot_comp_exp.py
python plot_comp_exp_reacher.py







